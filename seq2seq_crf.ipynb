{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"gk1vfWSVNvqf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649228953502,"user_tz":-480,"elapsed":21200,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"a99ff3e4-fa9d-4ba0-fcfe-435a00f47cb6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"dndQRFBTNPtY","executionInfo":{"status":"ok","timestamp":1649229161005,"user_tz":-480,"elapsed":307,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}}},"outputs":[],"source":["import xml.etree.ElementTree as ET\n","import pandas as pd\n"]},{"cell_type":"code","source":["def import_file_to_df(file_path):\n","  data = []\n","\n","  with open(file_path, 'r') as xml_file:\n","    tree = ET.parse(xml_file)\n","    sentences = tree.getroot()\n","\n","    for sent in sentences:\n","      record = dict()\n","      record[\"id\"] = sent.attrib['id']\n","      record[\"text\"] = sent.findall(\".//text\")[0].text\n","      record[\"aspectTerms\"] = []\n","      # record[\"aspectCats\"] = []\n","\n","      aspectTerms = sent.findall(\".//aspectTerms\")\n","      if aspectTerms:\n","        record[\"aspectTerms\"] = [term.attrib for term in sent.findall(\".//aspectTerms\")[0]]\n","\n","      # aspectCats = sent.findall(\".//aspectCategories\")\n","      # if aspectCats:\n","      #   record[\"aspectCats\"] = [cat.attrib for cat in sent.findall(\".//aspectCategories\")[0]]\n","\n","      data.append(record)\n","  return pd.DataFrame(data)"],"metadata":{"id":"H9DZgserOx34","executionInfo":{"status":"ok","timestamp":1649229163342,"user_tz":-480,"elapsed":2,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["df = import_file_to_df('drive/MyDrive/ABSA_data/Restaurants_Train.xml')\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"I0ZwXmzUc2-x","executionInfo":{"status":"ok","timestamp":1649090417289,"user_tz":-480,"elapsed":4,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"2a2b3750-a378-4ac9-da10-b16509f574de"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        id                                               text  \\\n","0     3121               But the staff was so horrible to us.   \n","1     2777  To be completely fair, the only redeeming fact...   \n","2     1634  The food is uniformly exceptional, with a very...   \n","3     2534  Where Gabriela personaly greets you and recomm...   \n","4      583  For those that go once and don't enjoy it, all...   \n","...    ...                                                ...   \n","3039  1063                     But that is highly forgivable.   \n","3040   777  From the appetizers we ate, the dim sum and ot...   \n","3041   875  When we arrived at 6:00 PM, the restaurant was...   \n","3042   671  Each table has a pot of boiling water sunken i...   \n","3043   617          I am going to the mid town location next.   \n","\n","                                            aspectTerms  \n","0     [{'term': 'staff', 'polarity': 'negative', 'fr...  \n","1     [{'term': 'food', 'polarity': 'positive', 'fro...  \n","2     [{'term': 'food', 'polarity': 'positive', 'fro...  \n","3                                                    []  \n","4                                                    []  \n","...                                                 ...  \n","3039                                                 []  \n","3040  [{'term': 'appetizers', 'polarity': 'positive'...  \n","3041                                                 []  \n","3042  [{'term': 'table', 'polarity': 'neutral', 'fro...  \n","3043                                                 []  \n","\n","[3044 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-497c9425-201f-45c4-a4f1-8313ac786575\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>aspectTerms</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3121</td>\n","      <td>But the staff was so horrible to us.</td>\n","      <td>[{'term': 'staff', 'polarity': 'negative', 'fr...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2777</td>\n","      <td>To be completely fair, the only redeeming fact...</td>\n","      <td>[{'term': 'food', 'polarity': 'positive', 'fro...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1634</td>\n","      <td>The food is uniformly exceptional, with a very...</td>\n","      <td>[{'term': 'food', 'polarity': 'positive', 'fro...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2534</td>\n","      <td>Where Gabriela personaly greets you and recomm...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>583</td>\n","      <td>For those that go once and don't enjoy it, all...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3039</th>\n","      <td>1063</td>\n","      <td>But that is highly forgivable.</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>3040</th>\n","      <td>777</td>\n","      <td>From the appetizers we ate, the dim sum and ot...</td>\n","      <td>[{'term': 'appetizers', 'polarity': 'positive'...</td>\n","    </tr>\n","    <tr>\n","      <th>3041</th>\n","      <td>875</td>\n","      <td>When we arrived at 6:00 PM, the restaurant was...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>3042</th>\n","      <td>671</td>\n","      <td>Each table has a pot of boiling water sunken i...</td>\n","      <td>[{'term': 'table', 'polarity': 'neutral', 'fro...</td>\n","    </tr>\n","    <tr>\n","      <th>3043</th>\n","      <td>617</td>\n","      <td>I am going to the mid town location next.</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3044 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-497c9425-201f-45c4-a4f1-8313ac786575')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-497c9425-201f-45c4-a4f1-8313ac786575 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-497c9425-201f-45c4-a4f1-8313ac786575');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["##POST"],"metadata":{"id":"iSYGhCsqWqnT"}},{"cell_type":"code","source":["!pip install --upgrade spacy\n","!python -m spacy download en_core_web_sm"],"metadata":{"id":"xVVqgNPzd8XI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy"],"metadata":{"id":"FM3fNA7GWug4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"DCJm5I3_Y-M1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens_series = []\n","tags_series = []\n","noun_chunks_series = []\n","# gonna take ~30s to run\n","for index, row in df.iterrows():\n","  sentence_doc = nlp(row['text'])\n","  tokens = []\n","  tags = []\n","  noun_chunks = []\n","  for token in sentence_doc:\n","    tokens.append(token.text)\n","    tags.append(token.tag_)\n","  for chunks in sentence_doc.noun_chunks:\n","    noun_chunks.append(chunks)\n","  # print('Toekns={}, tags={}'.format(tokens, tags))\n","  tokens_series.append(tokens)\n","  tags_series.append(tags)\n","  noun_chunks_series.append(noun_chunks)\n","  # print('row={}'.format(row))\n","df['tokens'] = tokens_series\n","df['tags'] = tags_series\n","df['noun_chunks'] = noun_chunks_series"],"metadata":{"id":"IwUUQzRPZFKo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"E4ohDWLshXV4","executionInfo":{"status":"ok","timestamp":1647078561757,"user_tz":-480,"elapsed":5,"user":{"displayName":"Zhenghao Wu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPQMClyyeie5fvbJhxkNzpIXFoXj0V98XBjn-vhek=s64","userId":"02686925948298387783"}},"outputId":"3d13cfee-72d8-4916-c8f7-fdf7a31f2c1d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        id                                               text  \\\n","0     3121               But the staff was so horrible to us.   \n","1     2777  To be completely fair, the only redeeming fact...   \n","2     1634  The food is uniformly exceptional, with a very...   \n","3     2534  Where Gabriela personaly greets you and recomm...   \n","4      583  For those that go once and don't enjoy it, all...   \n","...    ...                                                ...   \n","3039  1063                     But that is highly forgivable.   \n","3040   777  From the appetizers we ate, the dim sum and ot...   \n","3041   875  When we arrived at 6:00 PM, the restaurant was...   \n","3042   671  Each table has a pot of boiling water sunken i...   \n","3043   617          I am going to the mid town location next.   \n","\n","                                            aspectTerms  \\\n","0     [{'term': 'staff', 'polarity': 'negative', 'fr...   \n","1     [{'term': 'food', 'polarity': 'positive', 'fro...   \n","2     [{'term': 'food', 'polarity': 'positive', 'fro...   \n","3                                                    []   \n","4                                                    []   \n","...                                                 ...   \n","3039                                                 []   \n","3040  [{'term': 'appetizers', 'polarity': 'positive'...   \n","3041                                                 []   \n","3042  [{'term': 'table', 'polarity': 'neutral', 'fro...   \n","3043                                                 []   \n","\n","                                                 tokens  \\\n","0       [But, the, staff, was, so, horrible, to, us, .]   \n","1     [To, be, completely, fair, ,, the, only, redee...   \n","2     [The, food, is, uniformly, exceptional, ,, wit...   \n","3     [Where, Gabriela, personaly, greets, you, and,...   \n","4     [For, those, that, go, once, and, do, n't, enj...   \n","...                                                 ...   \n","3039             [But, that, is, highly, forgivable, .]   \n","3040  [From, the, appetizers, we, ate, ,, the, dim, ...   \n","3041  [When, we, arrived, at, 6:00, PM, ,, the, rest...   \n","3042  [Each, table, has, a, pot, of, boiling, water,...   \n","3043  [I, am, going, to, the, mid, town, location, n...   \n","\n","                                                   tags  \\\n","0                 [CC, DT, NN, VBD, RB, JJ, IN, PRP, .]   \n","1     [TO, VB, RB, JJ, ,, DT, JJ, VBG, NN, VBD, DT, ...   \n","2     [DT, NN, VBZ, RB, JJ, ,, IN, DT, RB, JJ, NN, W...   \n","3     [WRB, NNP, NNP, VBZ, PRP, CC, VBZ, PRP, WP, TO...   \n","4     [IN, DT, WDT, VBP, RB, CC, VBP, RB, VB, PRP, ,...   \n","...                                                 ...   \n","3039                           [CC, DT, VBZ, RB, JJ, .]   \n","3040  [IN, DT, NNS, PRP, VBD, ,, DT, JJ, NN, CC, JJ,...   \n","3041  [WRB, PRP, VBD, IN, CD, NN, ,, DT, NN, VBD, RB...   \n","3042  [DT, NN, VBZ, DT, NN, IN, NN, NN, VBN, IN, PRP...   \n","3043         [PRP, VBP, VBG, IN, DT, JJ, NN, NN, RB, .]   \n","\n","                                            noun_chunks  \n","0                                  [(the, staff), (us)]  \n","1     [(the, only, redeeming, factor), (the, food), ...  \n","2     [(The, food), (a, very, capable, kitchen), (wh...  \n","3         [(Gabriela, personaly), (you), (you), (what)]  \n","4     [(those), (that), (it), (all), (I), (they), (it)]  \n","...                                                 ...  \n","3039                                           [(that)]  \n","3040  [(the, appetizers), (we), (foods), (it), (the,...  \n","3041              [(we), (6:00, PM), (the, restaurant)]  \n","3042  [(Each, table), (a, pot), (boiling, water), (i...  \n","3043                  [(I), (the, mid, town, location)]  \n","\n","[3044 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-5101e7a6-121f-45ab-9a02-e9cb08e9d8b2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>aspectTerms</th>\n","      <th>tokens</th>\n","      <th>tags</th>\n","      <th>noun_chunks</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3121</td>\n","      <td>But the staff was so horrible to us.</td>\n","      <td>[{'term': 'staff', 'polarity': 'negative', 'fr...</td>\n","      <td>[But, the, staff, was, so, horrible, to, us, .]</td>\n","      <td>[CC, DT, NN, VBD, RB, JJ, IN, PRP, .]</td>\n","      <td>[(the, staff), (us)]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2777</td>\n","      <td>To be completely fair, the only redeeming fact...</td>\n","      <td>[{'term': 'food', 'polarity': 'positive', 'fro...</td>\n","      <td>[To, be, completely, fair, ,, the, only, redee...</td>\n","      <td>[TO, VB, RB, JJ, ,, DT, JJ, VBG, NN, VBD, DT, ...</td>\n","      <td>[(the, only, redeeming, factor), (the, food), ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1634</td>\n","      <td>The food is uniformly exceptional, with a very...</td>\n","      <td>[{'term': 'food', 'polarity': 'positive', 'fro...</td>\n","      <td>[The, food, is, uniformly, exceptional, ,, wit...</td>\n","      <td>[DT, NN, VBZ, RB, JJ, ,, IN, DT, RB, JJ, NN, W...</td>\n","      <td>[(The, food), (a, very, capable, kitchen), (wh...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2534</td>\n","      <td>Where Gabriela personaly greets you and recomm...</td>\n","      <td>[]</td>\n","      <td>[Where, Gabriela, personaly, greets, you, and,...</td>\n","      <td>[WRB, NNP, NNP, VBZ, PRP, CC, VBZ, PRP, WP, TO...</td>\n","      <td>[(Gabriela, personaly), (you), (you), (what)]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>583</td>\n","      <td>For those that go once and don't enjoy it, all...</td>\n","      <td>[]</td>\n","      <td>[For, those, that, go, once, and, do, n't, enj...</td>\n","      <td>[IN, DT, WDT, VBP, RB, CC, VBP, RB, VB, PRP, ,...</td>\n","      <td>[(those), (that), (it), (all), (I), (they), (it)]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3039</th>\n","      <td>1063</td>\n","      <td>But that is highly forgivable.</td>\n","      <td>[]</td>\n","      <td>[But, that, is, highly, forgivable, .]</td>\n","      <td>[CC, DT, VBZ, RB, JJ, .]</td>\n","      <td>[(that)]</td>\n","    </tr>\n","    <tr>\n","      <th>3040</th>\n","      <td>777</td>\n","      <td>From the appetizers we ate, the dim sum and ot...</td>\n","      <td>[{'term': 'appetizers', 'polarity': 'positive'...</td>\n","      <td>[From, the, appetizers, we, ate, ,, the, dim, ...</td>\n","      <td>[IN, DT, NNS, PRP, VBD, ,, DT, JJ, NN, CC, JJ,...</td>\n","      <td>[(the, appetizers), (we), (foods), (it), (the,...</td>\n","    </tr>\n","    <tr>\n","      <th>3041</th>\n","      <td>875</td>\n","      <td>When we arrived at 6:00 PM, the restaurant was...</td>\n","      <td>[]</td>\n","      <td>[When, we, arrived, at, 6:00, PM, ,, the, rest...</td>\n","      <td>[WRB, PRP, VBD, IN, CD, NN, ,, DT, NN, VBD, RB...</td>\n","      <td>[(we), (6:00, PM), (the, restaurant)]</td>\n","    </tr>\n","    <tr>\n","      <th>3042</th>\n","      <td>671</td>\n","      <td>Each table has a pot of boiling water sunken i...</td>\n","      <td>[{'term': 'table', 'polarity': 'neutral', 'fro...</td>\n","      <td>[Each, table, has, a, pot, of, boiling, water,...</td>\n","      <td>[DT, NN, VBZ, DT, NN, IN, NN, NN, VBN, IN, PRP...</td>\n","      <td>[(Each, table), (a, pot), (boiling, water), (i...</td>\n","    </tr>\n","    <tr>\n","      <th>3043</th>\n","      <td>617</td>\n","      <td>I am going to the mid town location next.</td>\n","      <td>[]</td>\n","      <td>[I, am, going, to, the, mid, town, location, n...</td>\n","      <td>[PRP, VBP, VBG, IN, DT, JJ, NN, NN, RB, .]</td>\n","      <td>[(I), (the, mid, town, location)]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3044 rows × 6 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5101e7a6-121f-45ab-9a02-e9cb08e9d8b2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5101e7a6-121f-45ab-9a02-e9cb08e9d8b2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5101e7a6-121f-45ab-9a02-e9cb08e9d8b2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## B-LSTM + CRF Model"],"metadata":{"id":"E9XwCb8aiXWs"}},{"cell_type":"markdown","source":["neurNER needs Tensorflow 1 (duh!), need to downgrade the Tensorflow 2 that comes with Colab"],"metadata":{"id":"FzZ80ovjhXGW"}},{"cell_type":"code","source":["%tensorflow_version 1.15.2\n","import tensorflow as tf\n","print(tf.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZZ0i6Z3hllX","executionInfo":{"status":"ok","timestamp":1649229172207,"user_tz":-480,"elapsed":6010,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"9a67d816-5aae-486a-e5e4-f55d7c63d103"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.15.2`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n","1.15.2\n"]}]},{"cell_type":"markdown","source":["Download neuroNER engine"],"metadata":{"id":"VluoWjtlfgFF"}},{"cell_type":"code","source":["!pip3 install pyneuroner[cpu]"],"metadata":{"id":"NmbfoFvSfPhA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649228980678,"user_tz":-480,"elapsed":18178,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"04e0f20e-87a9-46f7-9bbe-d7cf77387888"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyneuroner[cpu]\n","  Downloading pyneuroner-1.0.8-py2.py3-none-any.whl (26.9 MB)\n","\u001b[K     |████████████████████████████████| 26.9 MB 3.9 MB/s \n","\u001b[?25hCollecting pycorenlp>=0.3.0\n","  Downloading pycorenlp-0.3.0.tar.gz (1.3 kB)\n","Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.7/dist-packages (from pyneuroner[cpu]) (2.2.4)\n","Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from pyneuroner[cpu]) (3.2.2)\n","Requirement already satisfied: scikit-learn>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from pyneuroner[cpu]) (1.0.2)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from pyneuroner[cpu]) (2.6.3)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyneuroner[cpu]) (1.4.1)\n","Requirement already satisfied: tensorflow>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from pyneuroner[cpu]) (2.8.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->pyneuroner[cpu]) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->pyneuroner[cpu]) (3.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->pyneuroner[cpu]) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->pyneuroner[cpu]) (1.4.0)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->pyneuroner[cpu]) (1.21.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.2->pyneuroner[cpu]) (3.10.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pycorenlp>=0.3.0->pyneuroner[cpu]) (2.23.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.2->pyneuroner[cpu]) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.2->pyneuroner[cpu]) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.2->pyneuroner[cpu]) (1.1.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (0.9.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (7.4.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (1.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (57.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (2.0.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (3.0.6)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (1.1.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (4.63.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (1.0.6)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (1.0.5)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->pyneuroner[cpu]) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->pyneuroner[cpu]) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pycorenlp>=0.3.0->pyneuroner[cpu]) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pycorenlp>=0.3.0->pyneuroner[cpu]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pycorenlp>=0.3.0->pyneuroner[cpu]) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pycorenlp>=0.3.0->pyneuroner[cpu]) (2.10)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (1.44.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (3.3.0)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[K     |████████████████████████████████| 462 kB 39.9 MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (1.14.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (2.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (3.17.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (1.1.0)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (2.8.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (13.0.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (1.6.3)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (2.8.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (3.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (1.1.2)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (0.5.3)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (1.0.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (0.24.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=1.12.0->pyneuroner[cpu]) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=1.12.0->pyneuroner[cpu]) (1.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=1.12.0->pyneuroner[cpu]) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=1.12.0->pyneuroner[cpu]) (3.3.6)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=1.12.0->pyneuroner[cpu]) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=1.12.0->pyneuroner[cpu]) (1.35.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=1.12.0->pyneuroner[cpu]) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=1.12.0->pyneuroner[cpu]) (1.0.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=1.12.0->pyneuroner[cpu]) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=1.12.0->pyneuroner[cpu]) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=1.12.0->pyneuroner[cpu]) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=1.12.0->pyneuroner[cpu]) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=1.12.0->pyneuroner[cpu]) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=1.12.0->pyneuroner[cpu]) (3.2.0)\n","Building wheels for collected packages: pycorenlp\n","  Building wheel for pycorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycorenlp: filename=pycorenlp-0.3.0-py3-none-any.whl size=2145 sha256=1070d7619633e98bd60e6aac16f2aff948e207a71eaf61bd31fce174beda1375\n","  Stored in directory: /root/.cache/pip/wheels/83/d8/ad/6b2276343ac605ee47e6beddb28331e96377909e5c816539c3\n","Successfully built pycorenlp\n","Installing collected packages: tf-estimator-nightly, pycorenlp, pyneuroner\n","Successfully installed pycorenlp-0.3.0 pyneuroner-1.0.8 tf-estimator-nightly-2.8.0.dev2021122109\n"]}]},{"cell_type":"markdown","source":["Download the SpaCy English module"],"metadata":{"id":"caKj7knGflxW"}},{"cell_type":"code","source":["!python -m spacy download en"],"metadata":{"id":"46V1_lcWfXzm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649229008131,"user_tz":-480,"elapsed":13139,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"4ffbc7da-82cf-4a27-8171-043f89e26d2c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en_core_web_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n","\u001b[K     |████████████████████████████████| 12.0 MB 7.9 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n","/usr/local/lib/python3.7/dist-packages/spacy/data/en\n","You can now load the model via spacy.load('en')\n"]}]},{"cell_type":"markdown","source":["Download word embeddings. Original paper uses fastText embedding. We use Glove first. Can try fastText later."],"metadata":{"id":"4tCRa0hjfstV"}},{"cell_type":"code","source":["!wget -P data/word_vectors http://neuroner.com/data/word_vectors/glove.6B.100d.zip\n","!unzip data/word_vectors/glove.6B.100d.zip -d data/word_vectors/"],"metadata":{"id":"mGRWlfwTftoE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649229030967,"user_tz":-480,"elapsed":18493,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"bf1ba9db-3336-4ca0-ed18-03ba86c6e466"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-06 07:10:12--  http://neuroner.com/data/word_vectors/glove.6B.100d.zip\n","Resolving neuroner.com (neuroner.com)... 142.44.246.184\n","Connecting to neuroner.com (neuroner.com)|142.44.246.184|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 122612186 (117M) [application/zip]\n","Saving to: ‘data/word_vectors/glove.6B.100d.zip’\n","\n","glove.6B.100d.zip   100%[===================>] 116.93M  11.7MB/s    in 10s     \n","\n","2022-04-06 07:10:23 (11.3 MB/s) - ‘data/word_vectors/glove.6B.100d.zip’ saved [122612186/122612186]\n","\n","Archive:  data/word_vectors/glove.6B.100d.zip\n","  inflating: data/word_vectors/glove.6B.100d.txt  \n"]}]},{"cell_type":"markdown","source":["Some preparation work"],"metadata":{"id":"1UFkXk1VCIFw"}},{"cell_type":"code","source":["import numpy as np\n","# from keras.preprocessing import sequence\n","# from keras.models import Sequential\n","# from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n","from neuroner import neuromodel\n","import spacy\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"ktB-HjChibT5","executionInfo":{"status":"ok","timestamp":1649229177317,"user_tz":-480,"elapsed":5113,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"Hi-a2R5CibWc","executionInfo":{"status":"ok","timestamp":1649229182021,"user_tz":-480,"elapsed":3402,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Generate input file"],"metadata":{"id":"3nxFBbKrrInU"}},{"cell_type":"code","source":["def get_idx_iob(idx, range_list):\n","  # Use IOB2 format\n","  for range in range_list:\n","    if range[0] <= idx < range[1]:\n","      if range[0] == idx:\n","        return 'B-MISC'\n","      else:\n","        return 'I-MISC'\n","  return 'O'\n","\n","# CoNLL-2003 format files\n","# Each word to be put on a separate line and there is an empty line after each sentence.\n","# Each line  has format: <token> + <single space> + <iob tag>, e.g.: \"This O\"\n","def generate_input_file(path, filename, data_df):\n","  f = open(path + filename, \"x\")\n","  for index, row in data_df.iterrows():\n","    # A list of tuples, each tuple is of (<from>, <to>)\n","    aspectTermRanges = [(int(aspectTerm['from']), int(aspectTerm['to'])) for aspectTerm in row['aspectTerms']]\n","    sentence_doc = nlp(row['text'])\n","    for token in sentence_doc:\n","      if len(token.text.strip()) == 0:\n","        continue\n","      f.write(token.text + ' ' + get_idx_iob(token.idx, aspectTermRanges) + '\\n')\n","    f.write('\\n')\n","  f.close()"],"metadata":{"id":"FeqP1ZWrnURx","executionInfo":{"status":"ok","timestamp":1649229183907,"user_tz":-480,"elapsed":317,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["input_df = import_file_to_df('drive/MyDrive/ABSA_data/Restaurants_Train.xml')\n","test_df = import_file_to_df('drive/MyDrive/ABSA_data/Restaurants_Test.xml')"],"metadata":{"id":"xfaKuYxBh0Aa","executionInfo":{"status":"ok","timestamp":1649229189300,"user_tz":-480,"elapsed":1279,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["input_df = input_df.drop(input_df[input_df.text.str.contains('touchpad')].index)\n","test_df = test_df.drop(test_df[test_df.text.str.contains('touchpad')].index)\n","test_df = test_df.drop(test_df[test_df.text.str.contains('programs')].index)"],"metadata":{"id":"U5jXz4uv15Y8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df, validate_df = train_test_split(input_df, test_size=0.2)"],"metadata":{"id":"qlM2HbtwxZHU","executionInfo":{"status":"ok","timestamp":1649229049333,"user_tz":-480,"elapsed":2,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Gonna run for ~40s\n","generate_input_file('./', 'train.txt', train_df)\n","generate_input_file('./', 'valid.txt', validate_df)\n","generate_input_file('./', 'test.txt', test_df)"],"metadata":{"id":"0L1Cotf6yVbx","executionInfo":{"status":"ok","timestamp":1649229098171,"user_tz":-480,"elapsed":46914,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["!rm -r /content/valid*\n","!rm -r /content/train*\n","!rm -r /content/test*"],"metadata":{"id":"mv9rPfkNvAVE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's roll"],"metadata":{"id":"E9LsiXlvCNEW"}},{"cell_type":"code","source":["# Obtain a model, all configurable params: https://github.com/Franck-Dernoncourt/NeuroNER/blob/master/parameters.ini\n","# The parameters used here follow the original paper expect mini-batch SGD. This is because vanilla NeuroNER dose not support batch training (i.e. it runs with batch size = 1).\n","# The paper used mini-batch SGD with batch size 64. We need to modify NeuroNER source to support this.\n","# So as a baseline, use batch size = 1 first.\n","tf.reset_default_graph()\n","nn = neuromodel.NeuroNER(use_crf=True, use_character_lstm=False, train_model=True, use_pretrained_model=False, dataset_text_folder='./', optimizer='adam', learning_rate = 0.01)"],"metadata":{"id":"ltBdfFA5zHCX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649235431968,"user_tz":-480,"elapsed":31556,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"30189c68-3029-42e8-9bea-61037e8c397a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["{'character_embedding_dimension': 25,\n"," 'character_lstm_hidden_state_dimension': 25,\n"," 'check_for_digits_replaced_with_zeros': 1,\n"," 'check_for_lowercase': 1,\n"," 'dataset_text_folder': './',\n"," 'debug': 0,\n"," 'dropout_rate': 0.5,\n"," 'experiment_name': 'experiment',\n"," 'freeze_token_embeddings': 0,\n"," 'gradient_clipping_value': 5.0,\n"," 'learning_rate': 0.01,\n"," 'load_all_pretrained_token_embeddings': 0,\n"," 'load_only_pretrained_token_embeddings': 0,\n"," 'main_evaluation_mode': 'conll',\n"," 'maximum_number_of_epochs': 100,\n"," 'number_of_cpu_threads': 8,\n"," 'number_of_gpus': 0,\n"," 'optimizer': 'adam',\n"," 'output_folder': './output',\n"," 'output_scores': 0,\n"," 'parameters_filepath': './parameters.ini',\n"," 'patience': 10,\n"," 'plot_format': 'pdf',\n"," 'pretrained_model_folder': './trained_models/conll_2003_en',\n"," 'reload_character_embeddings': 1,\n"," 'reload_character_lstm': 1,\n"," 'reload_crf': 1,\n"," 'reload_feedforward': 1,\n"," 'reload_token_embeddings': 1,\n"," 'reload_token_lstm': 1,\n"," 'remap_unknown_tokens_to_unk': 1,\n"," 'spacylanguage': 'en',\n"," 'tagging_format': 'bioes',\n"," 'token_embedding_dimension': 100,\n"," 'token_lstm_hidden_state_dimension': 100,\n"," 'token_pretrained_embedding_filepath': './data/word_vectors/glove.6B.100d.txt',\n"," 'tokenizer': 'spacy',\n"," 'train_model': 1,\n"," 'use_character_lstm': 0,\n"," 'use_crf': 1,\n"," 'use_pretrained_model': 0,\n"," 'verbose': 0}\n","Checking the validity of BRAT-formatted train set... Done.\n","Checking compatibility between CONLL and BRAT for train_compatible_with_brat set ... Done.\n","Checking validity of CONLL BIOES format... Done.\n","Checking the validity of BRAT-formatted valid set... Done.\n","Checking compatibility between CONLL and BRAT for valid_compatible_with_brat set ... Done.\n","Checking validity of CONLL BIOES format... Done.\n","Checking the validity of BRAT-formatted test set... Done.\n","Checking compatibility between CONLL and BRAT for test_compatible_with_brat set ... Done.\n","Checking validity of CONLL BIOES format... Done.\n","Load dataset... done (26.50 seconds)\n","Load token embeddings... done (0.19 seconds)\n","number_of_token_original_case_found: 4270\n","number_of_token_lowercase_found: 1417\n","number_of_token_digits_replaced_with_zeros_found: 1\n","number_of_token_lowercase_and_digits_replaced_with_zeros_found: 0\n","number_of_loaded_word_vectors: 5688\n","dataset.vocabulary_size: 5988\n"]}]},{"cell_type":"code","source":["nn.fit()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T96YWovoAvAc","outputId":"7f0aaba0-1340-4794-dfa9-998264801dda","executionInfo":{"status":"ok","timestamp":1649236666950,"user_tz":-480,"elapsed":1231791,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting epoch 0\n","Training completed in 0.00 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 22177 phrases; correct: 1153.\n","accuracy:  32.79%; precision:   5.20%; recall:  38.95%; FB1:   9.17\n","             MISC: precision:   5.20%; recall:  38.95%; FB1:   9.17  22177\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 5475 phrases; correct: 301.\n","accuracy:  33.01%; precision:   5.50%; recall:  40.73%; FB1:   9.69\n","             MISC: precision:   5.50%; recall:  40.73%; FB1:   9.69  5475\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 7409 phrases; correct: 445.\n","accuracy:  33.16%; precision:   6.01%; recall:  39.24%; FB1:  10.42\n","             MISC: precision:   6.01%; recall:  39.24%; FB1:  10.42  7409\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/000_train.txt > ./output/_2022-04-06_08-57-15-219265/000_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/000_valid.txt > ./output/_2022-04-06_08-57-15-219265/000_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/000_test.txt > ./output/_2022-04-06_08-57-15-219265/000_test.txt_conll_evaluation.txt\n","Formatting 000_train set from CONLL to BRAT... Done.\n","Formatting 000_valid set from CONLL to BRAT... Done.\n","Formatting 000_test set from CONLL to BRAT... Done.\n","The last 0 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 1\n","Training completed in 62.22 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 2591 phrases; correct: 2143.\n","accuracy:  96.30%; precision:  82.71%; recall:  72.40%; FB1:  77.21\n","             MISC: precision:  82.71%; recall:  72.40%; FB1:  77.21  2591\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 671 phrases; correct: 516.\n","accuracy:  95.21%; precision:  76.90%; recall:  69.82%; FB1:  73.19\n","             MISC: precision:  76.90%; recall:  69.82%; FB1:  73.19  671\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 1063 phrases; correct: 830.\n","accuracy:  95.10%; precision:  78.08%; recall:  73.19%; FB1:  75.56\n","             MISC: precision:  78.08%; recall:  73.19%; FB1:  75.56  1063\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/001_train.txt > ./output/_2022-04-06_08-57-15-219265/001_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/001_valid.txt > ./output/_2022-04-06_08-57-15-219265/001_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/001_test.txt > ./output/_2022-04-06_08-57-15-219265/001_test.txt_conll_evaluation.txt\n","Formatting 001_train set from CONLL to BRAT... Done.\n","Formatting 001_valid set from CONLL to BRAT... Done.\n","Formatting 001_test set from CONLL to BRAT... Done.\n","The last 0 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 2\n","Training completed in 57.89 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 2866 phrases; correct: 2287.\n","accuracy:  96.33%; precision:  79.80%; recall:  77.26%; FB1:  78.51\n","             MISC: precision:  79.80%; recall:  77.26%; FB1:  78.51  2866\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 775 phrases; correct: 555.\n","accuracy:  95.18%; precision:  71.61%; recall:  75.10%; FB1:  73.32\n","             MISC: precision:  71.61%; recall:  75.10%; FB1:  73.32  775\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 1229 phrases; correct: 894.\n","accuracy:  95.08%; precision:  72.74%; recall:  78.84%; FB1:  75.67\n","             MISC: precision:  72.74%; recall:  78.84%; FB1:  75.67  1229\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/002_train.txt > ./output/_2022-04-06_08-57-15-219265/002_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/002_valid.txt > ./output/_2022-04-06_08-57-15-219265/002_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/002_test.txt > ./output/_2022-04-06_08-57-15-219265/002_test.txt_conll_evaluation.txt\n","Formatting 002_train set from CONLL to BRAT... Done.\n","Formatting 002_valid set from CONLL to BRAT... Done.\n","Formatting 002_test set from CONLL to BRAT... Done.\n","The last 0 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 3\n","Training completed in 60.98 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 3038 phrases; correct: 2215.\n","accuracy:  95.54%; precision:  72.91%; recall:  74.83%; FB1:  73.86\n","             MISC: precision:  72.91%; recall:  74.83%; FB1:  73.86  3038\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 867 phrases; correct: 584.\n","accuracy:  94.96%; precision:  67.36%; recall:  79.03%; FB1:  72.73\n","             MISC: precision:  67.36%; recall:  79.03%; FB1:  72.73  867\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 1330 phrases; correct: 897.\n","accuracy:  94.32%; precision:  67.44%; recall:  79.10%; FB1:  72.81\n","             MISC: precision:  67.44%; recall:  79.10%; FB1:  72.81  1330\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/003_train.txt > ./output/_2022-04-06_08-57-15-219265/003_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/003_valid.txt > ./output/_2022-04-06_08-57-15-219265/003_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/003_test.txt > ./output/_2022-04-06_08-57-15-219265/003_test.txt_conll_evaluation.txt\n","The last 1 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 4\n","Training completed in 58.63 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 4170 phrases; correct: 2232.\n","accuracy:  91.92%; precision:  53.53%; recall:  75.41%; FB1:  62.61\n","             MISC: precision:  53.53%; recall:  75.41%; FB1:  62.61  4170\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 894 phrases; correct: 520.\n","accuracy:  92.13%; precision:  58.17%; recall:  70.37%; FB1:  63.69\n","             MISC: precision:  58.17%; recall:  70.37%; FB1:  63.69  894\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 1385 phrases; correct: 854.\n","accuracy:  91.85%; precision:  61.66%; recall:  75.31%; FB1:  67.80\n","             MISC: precision:  61.66%; recall:  75.31%; FB1:  67.80  1385\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/004_train.txt > ./output/_2022-04-06_08-57-15-219265/004_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/004_valid.txt > ./output/_2022-04-06_08-57-15-219265/004_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/004_test.txt > ./output/_2022-04-06_08-57-15-219265/004_test.txt_conll_evaluation.txt\n","The last 2 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 5\n","Training completed in 60.06 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 2690 phrases; correct: 2092.\n","accuracy:  95.47%; precision:  77.77%; recall:  70.68%; FB1:  74.05\n","             MISC: precision:  77.77%; recall:  70.68%; FB1:  74.05  2690\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 718 phrases; correct: 532.\n","accuracy:  94.98%; precision:  74.09%; recall:  71.99%; FB1:  73.03\n","             MISC: precision:  74.09%; recall:  71.99%; FB1:  73.03  718\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 1106 phrases; correct: 813.\n","accuracy:  94.28%; precision:  73.51%; recall:  71.69%; FB1:  72.59\n","             MISC: precision:  73.51%; recall:  71.69%; FB1:  72.59  1106\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/005_train.txt > ./output/_2022-04-06_08-57-15-219265/005_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/005_valid.txt > ./output/_2022-04-06_08-57-15-219265/005_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/005_test.txt > ./output/_2022-04-06_08-57-15-219265/005_test.txt_conll_evaluation.txt\n","The last 3 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 6\n","Training completed in 60.32 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 2445 phrases; correct: 2007.\n","accuracy:  95.74%; precision:  82.09%; recall:  67.80%; FB1:  74.26\n","             MISC: precision:  82.09%; recall:  67.80%; FB1:  74.26  2445\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 642 phrases; correct: 492.\n","accuracy:  94.91%; precision:  76.64%; recall:  66.58%; FB1:  71.25\n","             MISC: precision:  76.64%; recall:  66.58%; FB1:  71.25  642\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 992 phrases; correct: 756.\n","accuracy:  94.43%; precision:  76.21%; recall:  66.67%; FB1:  71.12\n","             MISC: precision:  76.21%; recall:  66.67%; FB1:  71.12  992\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/006_train.txt > ./output/_2022-04-06_08-57-15-219265/006_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/006_valid.txt > ./output/_2022-04-06_08-57-15-219265/006_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/006_test.txt > ./output/_2022-04-06_08-57-15-219265/006_test.txt_conll_evaluation.txt\n","The last 4 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 7\n","Training completed in 58.34 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 2731 phrases; correct: 2139.\n","accuracy:  95.73%; precision:  78.32%; recall:  72.26%; FB1:  75.17\n","             MISC: precision:  78.32%; recall:  72.26%; FB1:  75.17  2731\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 750 phrases; correct: 549.\n","accuracy:  95.09%; precision:  73.20%; recall:  74.29%; FB1:  73.74\n","             MISC: precision:  73.20%; recall:  74.29%; FB1:  73.74  750\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 1148 phrases; correct: 831.\n","accuracy:  94.23%; precision:  72.39%; recall:  73.28%; FB1:  72.83\n","             MISC: precision:  72.39%; recall:  73.28%; FB1:  72.83  1148\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/007_train.txt > ./output/_2022-04-06_08-57-15-219265/007_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/007_valid.txt > ./output/_2022-04-06_08-57-15-219265/007_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/007_test.txt > ./output/_2022-04-06_08-57-15-219265/007_test.txt_conll_evaluation.txt\n","Formatting 007_train set from CONLL to BRAT... Done.\n","Formatting 007_valid set from CONLL to BRAT... Done.\n","Formatting 007_test set from CONLL to BRAT... Done.\n","The last 0 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 8\n","Training completed in 59.97 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 2976 phrases; correct: 2209.\n","accuracy:  95.39%; precision:  74.23%; recall:  74.63%; FB1:  74.43\n","             MISC: precision:  74.23%; recall:  74.63%; FB1:  74.43  2976\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 764 phrases; correct: 551.\n","accuracy:  95.05%; precision:  72.12%; recall:  74.56%; FB1:  73.32\n","             MISC: precision:  72.12%; recall:  74.56%; FB1:  73.32  764\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 1187 phrases; correct: 854.\n","accuracy:  94.51%; precision:  71.95%; recall:  75.31%; FB1:  73.59\n","             MISC: precision:  71.95%; recall:  75.31%; FB1:  73.59  1187\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/008_train.txt > ./output/_2022-04-06_08-57-15-219265/008_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/008_valid.txt > ./output/_2022-04-06_08-57-15-219265/008_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/008_test.txt > ./output/_2022-04-06_08-57-15-219265/008_test.txt_conll_evaluation.txt\n","The last 1 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 9\n","Training completed in 60.06 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 2847 phrases; correct: 2225.\n","accuracy:  95.70%; precision:  78.15%; recall:  75.17%; FB1:  76.63\n","             MISC: precision:  78.15%; recall:  75.17%; FB1:  76.63  2847\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 789 phrases; correct: 556.\n","accuracy:  94.95%; precision:  70.47%; recall:  75.24%; FB1:  72.77\n","             MISC: precision:  70.47%; recall:  75.24%; FB1:  72.77  789\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 1195 phrases; correct: 849.\n","accuracy:  94.43%; precision:  71.05%; recall:  74.87%; FB1:  72.91\n","             MISC: precision:  71.05%; recall:  74.87%; FB1:  72.91  1195\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/009_train.txt > ./output/_2022-04-06_08-57-15-219265/009_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/009_valid.txt > ./output/_2022-04-06_08-57-15-219265/009_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/009_test.txt > ./output/_2022-04-06_08-57-15-219265/009_test.txt_conll_evaluation.txt\n","The last 2 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 10\n","Training completed in 59.86 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 2626 phrases; correct: 2118.\n","accuracy:  95.80%; precision:  80.65%; recall:  71.55%; FB1:  75.83\n","             MISC: precision:  80.65%; recall:  71.55%; FB1:  75.83  2626\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 696 phrases; correct: 540.\n","accuracy:  95.44%; precision:  77.59%; recall:  73.07%; FB1:  75.26\n","             MISC: precision:  77.59%; recall:  73.07%; FB1:  75.26  696\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 1063 phrases; correct: 814.\n","accuracy:  94.76%; precision:  76.58%; recall:  71.78%; FB1:  74.10\n","             MISC: precision:  76.58%; recall:  71.78%; FB1:  74.10  1063\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/010_train.txt > ./output/_2022-04-06_08-57-15-219265/010_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/010_valid.txt > ./output/_2022-04-06_08-57-15-219265/010_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/010_test.txt > ./output/_2022-04-06_08-57-15-219265/010_test.txt_conll_evaluation.txt\n","Formatting 010_train set from CONLL to BRAT... Done.\n","Formatting 010_valid set from CONLL to BRAT... Done.\n","Formatting 010_test set from CONLL to BRAT... Done.\n","The last 0 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 11\n","Training completed in 60.76 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 3198 phrases; correct: 2233.\n","accuracy:  95.13%; precision:  69.82%; recall:  75.44%; FB1:  72.52\n","             MISC: precision:  69.82%; recall:  75.44%; FB1:  72.52  3198\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 862 phrases; correct: 580.\n","accuracy:  94.95%; precision:  67.29%; recall:  78.48%; FB1:  72.45\n","             MISC: precision:  67.29%; recall:  78.48%; FB1:  72.45  862\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 1318 phrases; correct: 873.\n","accuracy:  94.12%; precision:  66.24%; recall:  76.98%; FB1:  71.21\n","             MISC: precision:  66.24%; recall:  76.98%; FB1:  71.21  1318\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/011_train.txt > ./output/_2022-04-06_08-57-15-219265/011_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/011_valid.txt > ./output/_2022-04-06_08-57-15-219265/011_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/011_test.txt > ./output/_2022-04-06_08-57-15-219265/011_test.txt_conll_evaluation.txt\n","The last 1 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 12\n","Training completed in 58.61 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 2770 phrases; correct: 2125.\n","accuracy:  95.50%; precision:  76.71%; recall:  71.79%; FB1:  74.17\n","             MISC: precision:  76.71%; recall:  71.79%; FB1:  74.17  2770\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 744 phrases; correct: 519.\n","accuracy:  94.50%; precision:  69.76%; recall:  70.23%; FB1:  69.99\n","             MISC: precision:  69.76%; recall:  70.23%; FB1:  69.99  744\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 1141 phrases; correct: 803.\n","accuracy:  93.85%; precision:  70.38%; recall:  70.81%; FB1:  70.59\n","             MISC: precision:  70.38%; recall:  70.81%; FB1:  70.59  1141\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/012_train.txt > ./output/_2022-04-06_08-57-15-219265/012_train.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/012_valid.txt > ./output/_2022-04-06_08-57-15-219265/012_valid.txt_conll_evaluation.txt\n","/usr/local/lib/python3.7/dist-packages/neuroner\n","shell_command: perl /usr/local/lib/python3.7/dist-packages/neuroner/conlleval < ./output/_2022-04-06_08-57-15-219265/012_test.txt > ./output/_2022-04-06_08-57-15-219265/012_test.txt_conll_evaluation.txt\n","The last 2 epochs have not shown improvements on the validation set.\n","\n","Starting epoch 13\n","Training completed in 61.49 seconds\n","Evaluate model on the train set\n","processed 38203 tokens with 2960 phrases; found: 2662 phrases; correct: 2031.\n","accuracy:  95.31%; precision:  76.30%; recall:  68.61%; FB1:  72.25\n","             MISC: precision:  76.30%; recall:  68.61%; FB1:  72.25  2662\n","\n","Evaluate model on the valid set\n","processed 9378 tokens with 739 phrases; found: 714 phrases; correct: 489.\n","accuracy:  94.12%; precision:  68.49%; recall:  66.17%; FB1:  67.31\n","             MISC: precision:  68.49%; recall:  66.17%; FB1:  67.31  714\n","\n","Evaluate model on the test set\n","processed 12758 tokens with 1134 phrases; found: 1102 phrases; correct: 772.\n","accuracy:  93.52%; precision:  70.05%; recall:  68.08%; FB1:  69.05\n","             MISC: precision:  70.05%; recall:  68.08%; FB1:  69.05  1102\n","\n","Generating plots for the train set\n","Generating plots for the valid set\n","Generating plots for the test set\n","Training interrupted\n","Finishing the experiment\n"]}]},{"cell_type":"markdown","source":["Glove + single b-LSTM + CRF: \n","\n","\n","*   Restaurants: 60s, acc-0.9559, f1-0.7743\n","*   laptop: 57.23s, acc-0.9513, f1-0.6086\n","\n","Glove and bi-LSTM char embedding + single b-LSTM + CRF: \n","\n","\n","*   Restaurants: 76s, acc-0.9485, f1-0.7417\n","*   laptop: 77.39s, acc-0.9475, f1-0.6133\n","\n"],"metadata":{"id":"-M-KuPTedlGG"}},{"cell_type":"markdown","source":["To save model, change the param and run the cell below"],"metadata":{"id":"VOxPs95vKBDw"}},{"cell_type":"code","source":["import glob\n","import os\n","import pickle\n","from pprint import pprint\n","import shutil\n","import neuroner.utils as utils\n","\n","from neuroner.entity_lstm import EntityLSTM\n","import tensorflow as tf\n","from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n","\n","from neuroner import utils_tf\n","from neuroner import neuromodel\n","\n","def trim_dataset_pickle(input_dataset_filepath, output_dataset_filepath=None, delete_token_mappings=False):\n","    '''\n","    Remove the dataset and labels from dataset.pickle. \n","    If delete_token_mappings = True, then also remove token_to_index and index_to_token except for UNK.\n","    '''\n","    print(\"Trimming dataset.pickle..\")\n","    if output_dataset_filepath == None:\n","        output_dataset_filepath = os.path.join(os.path.dirname(input_dataset_filepath), \n","            'dataset_trimmed.pickle')\n","    dataset = pickle.load(open(input_dataset_filepath, 'rb'))\n","    count = 0\n","    print(\"Keys removed:\")\n","    keys_to_remove = ['character_indices', 'character_indices_padded', 'characters', \n","        'label_indices', 'label_vector_indices', 'labels', 'token_indices', \n","        'token_lengths', 'tokens', 'infrequent_token_indices', 'tokens_mapped_to_unk']\n","    for key in keys_to_remove:\n","        if key in dataset.__dict__:\n","            del dataset.__dict__[key]\n","            print('\\t' + key)\n","            count += 1            \n","    if delete_token_mappings:\n","        dataset.__dict__['token_to_index'] = {dataset.__dict__['UNK']:dataset.__dict__['UNK_TOKEN_INDEX']}\n","        dataset.__dict__['index_to_token'] = {dataset.__dict__['UNK_TOKEN_INDEX']:dataset.__dict__['UNK']}\n","    print(\"Number of keys removed: {0}\".format(count))\n","    pprint(dataset.__dict__)\n","    pickle.dump(dataset, open(output_dataset_filepath, 'wb'))\n","    print(\"Done!\")\n","\n","\n","def trim_model_checkpoint(parameters_filepath, dataset_filepath, input_checkpoint_filepath, \n","    output_checkpoint_filepath):\n","    '''\n","    Remove all token embeddings except UNK.\n","    '''\n","    parameters, _ = neuromodel.load_parameters(parameters_filepath=parameters_filepath)\n","    dataset = pickle.load(open(dataset_filepath, 'rb'))\n","    model = EntityLSTM(dataset, parameters) \n","    with tf.Session() as sess:\n","        model_saver = tf.train.Saver()  # defaults to saving all variables\n","        \n","        # Restore the pretrained model\n","        model_saver.restore(sess, input_checkpoint_filepath) # Works only when the dimensions of tensor variables are matched.\n","        \n","        # Get pretrained embeddings\n","        token_embedding_weights = sess.run(model.token_embedding_weights) \n","    \n","        # Restore the sizes of token embedding weights\n","        utils_tf.resize_tensor_variable(sess, model.token_embedding_weights, \n","            [1, parameters['token_embedding_dimension']]) \n","            \n","        initial_weights = sess.run(model.token_embedding_weights)\n","        initial_weights[dataset.UNK_TOKEN_INDEX] = token_embedding_weights[dataset.UNK_TOKEN_INDEX]\n","        sess.run(tf.assign(model.token_embedding_weights, initial_weights, validate_shape=False))\n","    \n","        token_embedding_weights = sess.run(model.token_embedding_weights) \n","        print(\"token_embedding_weights: {0}\".format(token_embedding_weights))\n","        \n","        model_saver.save(sess, output_checkpoint_filepath)\n","            \n","    dataset.__dict__['vocabulary_size'] = 1\n","    pickle.dump(dataset, open(dataset_filepath, 'wb'))\n","    pprint(dataset.__dict__)\n","\n","\n","def prepare_pretrained_model_for_restoring(output_folder_name, epoch_number, \n","    model_name, delete_token_mappings=False):\n","    '''\n","    Copy the dataset.pickle, parameters.ini, and model checkpoint files after \n","    removing the data used for training.\n","    \n","    The dataset and labels are deleted from dataset.pickle by default. The only \n","    information about the dataset that remain in the pretrained model\n","    is the list of tokens that appears in the dataset and the corresponding token \n","    embeddings learned from the dataset.\n","    \n","    If delete_token_mappings is set to True, index_to_token and token_to_index \n","    mappings are deleted from dataset.pickle additionally,\n","    and the corresponding token embeddings are deleted from the model checkpoint \n","    files. In this case, the pretrained model would not contain\n","    any information about the dataset used for training the model. \n","    \n","    If you wish to share a pretrained model with delete_token_mappings = True, \n","    it is highly recommended to use some external pre-trained token \n","    embeddings and freeze them while training the model to obtain high performance. \n","    This can be done by specifying the token_pretrained_embedding_filepath \n","    and setting freeze_token_embeddings = True in parameters.ini for training.\n","    '''\n","    input_model_folder = os.path.join('.', 'output', output_folder_name, 'model')\n","    output_model_folder = os.path.join('.', 'trained_models', model_name)\n","    utils.create_folder_if_not_exists(output_model_folder)\n","\n","    # trim and copy dataset.pickle\n","    input_dataset_filepath = os.path.join(input_model_folder, 'dataset.pickle')\n","    output_dataset_filepath = os.path.join(output_model_folder, 'dataset.pickle')\n","    trim_dataset_pickle(input_dataset_filepath, output_dataset_filepath, \n","        delete_token_mappings=delete_token_mappings)\n","    \n","    # copy parameters.ini\n","    parameters_filepath = os.path.join(input_model_folder, 'parameters.ini')\n","    shutil.copy(parameters_filepath, output_model_folder)\n","    \n","    # (trim and) copy checkpoint files\n","    epoch_number_string = str(epoch_number).zfill(5)\n","    if delete_token_mappings:\n","        input_checkpoint_filepath = os.path.join(input_model_folder, \n","            'model_{0}.ckpt'.format(epoch_number_string))\n","        output_checkpoint_filepath = os.path.join(output_model_folder, 'model.ckpt')\n","        trim_model_checkpoint(parameters_filepath, output_dataset_filepath, \n","            input_checkpoint_filepath, output_checkpoint_filepath)\n","    else:\n","        for filepath in glob.glob(os.path.join(input_model_folder, \n","            'model_{0}.ckpt*'.format(epoch_number_string))):\n","            shutil.copyfile(filepath, os.path.join(output_model_folder, \n","                os.path.basename(filepath).replace('_' + epoch_number_string, '')))\n","\n"," \n","def check_contents_of_dataset_and_model_checkpoint(model_folder):\n","    '''\n","    Check the contents of dataset.pickle and model_xxx.ckpt.\n","    model_folder: folder containing dataset.pickle and model_xxx.ckpt to be checked. \n","    '''\n","    dataset_filepath = os.path.join(model_folder, 'dataset.pickle')\n","    dataset = pickle.load(open(dataset_filepath, 'rb'))\n","    pprint(dataset.__dict__)\n","    pprint(list(dataset.__dict__.keys()))\n","\n","    checkpoint_filepath = os.path.join(model_folder, 'model.ckpt')\n","    with tf.Session() as sess:\n","        print_tensors_in_checkpoint_file(checkpoint_filepath, \n","            tensor_name='token_embedding/token_embedding_weights', all_tensors=True)\n","        print_tensors_in_checkpoint_file(checkpoint_filepath, \n","            tensor_name='token_embedding/token_embedding_weights', all_tensors=False)\n","\n","\n","if __name__ == '__main__':\n","    output_folder_name = '_2022-03-12_16-42-57-891711'\n","    epoch_number = 7\n","    model_name = 'default_glove'\n","    delete_token_mappings = False\n","    prepare_pretrained_model_for_restoring(output_folder_name, epoch_number, \n","        model_name, delete_token_mappings)"],"metadata":{"id":"wh0uCSxqH6Id"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r ./trained_model.zip /content/trained_models/\n","from google.colab import files\n","files.download(\"./trained_model.zip\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"id":"UYCqpd0xJVXV","executionInfo":{"status":"ok","timestamp":1647105751717,"user_tz":-480,"elapsed":1189,"user":{"displayName":"Zhenghao Wu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPQMClyyeie5fvbJhxkNzpIXFoXj0V98XBjn-vhek=s64","userId":"02686925948298387783"}},"outputId":"34fb591c-26dc-4299-ca94-da6ac489e437"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/trained_models/ (stored 0%)\n","  adding: content/trained_models/default_glove/ (stored 0%)\n","  adding: content/trained_models/default_glove/model.ckpt.index (deflated 53%)\n","  adding: content/trained_models/default_glove/parameters.ini (stored 0%)\n","  adding: content/trained_models/default_glove/model.ckpt.data-00000-of-00001 (deflated 32%)\n","  adding: content/trained_models/default_glove/dataset.pickle (deflated 52%)\n","  adding: content/trained_models/default_glove/model.ckpt.meta (deflated 40%)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_14f2c6cc-bedc-4240-8608-45e6618e2d8b\", \"trained_model.zip\", 7716391)"]},"metadata":{}}]},{"cell_type":"code","source":["# Generate result table\n","import re\n","\n","test_predict_filename = 'test.txt'\n","train_predict_filename = 'train.txt'\n","model_num = '001_'\n","run_num = '_2022-04-06_08-57-15-219265'\n","\n","c_tokens = []\n","c_truth = []\n","c_prediction = []\n","result_df = pd.DataFrame(data={'case':[], 'truth':[], 'prediction':[]})\n","\n","with open('/content/output/' + run_num + '/' + model_num + test_predict_filename, 'r') as output_file:\n","  lines = output_file.readlines()\n","  for line in lines:\n","    if len(line.strip()) == 0:\n","      sentence = ' '.join(c_tokens)\n","      \n","      true_aspect = ' '.join(c_truth)\n","      true_aspect = re.sub(r\"(, ?)+\",',',true_aspect)\n","      true_aspect = re.sub(r\"^, ?| ?, ?$\",\"\", true_aspect)\n","      predict_aspect = ' '.join(c_prediction)\n","      predict_aspect = re.sub(r\"(, ?)+\",',',predict_aspect)\n","      predict_aspect = re.sub(r\"^, ?| ?, ?$\",\"\", predict_aspect)\n","      result_df = result_df.append({'case':sentence, 'truth':re.sub(r',+',',',true_aspect), 'prediction':re.sub(r',+',',',predict_aspect)}, ignore_index=True)\n","      c_tokens=[]\n","      c_truth = []\n","      c_prediction = []\n","      continue\n","    values = line.split()\n","    c_tokens.append(values[0])\n","    if values[-2][0] != 'O':\n","      c_truth.append(values[0])\n","    else:\n","      c_truth.append(',')\n","    if values[-1][0] != 'O':\n","      c_prediction.append(values[0])\n","    else:\n","      c_prediction.append(',')\n","\n"],"metadata":{"id":"zgxzpPGXbbVV","executionInfo":{"status":"ok","timestamp":1649236696276,"user_tz":-480,"elapsed":2195,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["result_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"bzxTjT_QeibC","executionInfo":{"status":"ok","timestamp":1649236699449,"user_tz":-480,"elapsed":1031,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"1cb03a42-37fb-4d2c-a820-3f28ec1535fc"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  case  \\\n","0                     The bread is top notch as well .   \n","1    I have to say they have one of the fastest del...   \n","2         Food is always fresh and hot- ready to eat !   \n","3       Did I mention that the coffee is OUTSTANDING ?   \n","4    Certainly not the best sushi in New York , how...   \n","..                                                 ...   \n","795                      Anyway , the owner was fake .   \n","796               Owner is pleasant and entertaining .   \n","797  I have never in my life sent back food before ...   \n","798  Although the restaurant itself is nice , I pre...   \n","799  Creamy appetizers -- taramasalata , eggplant s...   \n","\n","                                                 truth  \\\n","0                                                bread   \n","1                                       delivery times   \n","2                                                 Food   \n","3                                               coffee   \n","4                                         sushi ,place   \n","..                                                 ...   \n","795                                              owner   \n","796                                              Owner   \n","797                                       food ,waiter   \n","798                                               food   \n","799  Creamy appetizers ,taramasalata ,eggplant sala...   \n","\n","                                            prediction  \n","0                                                bread  \n","1                                             delivery  \n","2                                                 Food  \n","3                                               coffee  \n","4                                                sushi  \n","..                                                 ...  \n","795                                              owner  \n","796                                                     \n","797                                       food ,waiter  \n","798                                               food  \n","799  Creamy appetizers ,eggplant salad ,Greek yogur...  \n","\n","[800 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-cc9f6384-90d6-4e8d-a348-323305c14a23\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>case</th>\n","      <th>truth</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The bread is top notch as well .</td>\n","      <td>bread</td>\n","      <td>bread</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I have to say they have one of the fastest del...</td>\n","      <td>delivery times</td>\n","      <td>delivery</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Food is always fresh and hot- ready to eat !</td>\n","      <td>Food</td>\n","      <td>Food</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Did I mention that the coffee is OUTSTANDING ?</td>\n","      <td>coffee</td>\n","      <td>coffee</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Certainly not the best sushi in New York , how...</td>\n","      <td>sushi ,place</td>\n","      <td>sushi</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>795</th>\n","      <td>Anyway , the owner was fake .</td>\n","      <td>owner</td>\n","      <td>owner</td>\n","    </tr>\n","    <tr>\n","      <th>796</th>\n","      <td>Owner is pleasant and entertaining .</td>\n","      <td>Owner</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>797</th>\n","      <td>I have never in my life sent back food before ...</td>\n","      <td>food ,waiter</td>\n","      <td>food ,waiter</td>\n","    </tr>\n","    <tr>\n","      <th>798</th>\n","      <td>Although the restaurant itself is nice , I pre...</td>\n","      <td>food</td>\n","      <td>food</td>\n","    </tr>\n","    <tr>\n","      <th>799</th>\n","      <td>Creamy appetizers -- taramasalata , eggplant s...</td>\n","      <td>Creamy appetizers ,taramasalata ,eggplant sala...</td>\n","      <td>Creamy appetizers ,eggplant salad ,Greek yogur...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>800 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc9f6384-90d6-4e8d-a348-323305c14a23')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cc9f6384-90d6-4e8d-a348-323305c14a23 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cc9f6384-90d6-4e8d-a348-323305c14a23');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["result_df.to_csv('test_result_labels.csv')"],"metadata":{"id":"0CWMMZ_letYK","executionInfo":{"status":"ok","timestamp":1649236707221,"user_tz":-480,"elapsed":2,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# No embedding, b-LSTM + CRF"],"metadata":{"id":"gldEImOIAuz_"}},{"cell_type":"code","source":["!pip install git+https://www.github.com/keras-team/keras-contrib.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hGZV68laUgya","executionInfo":{"status":"ok","timestamp":1648889393831,"user_tz":-480,"elapsed":3822,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"ff4fd491-d3f9-4533-ce96-c3d17d17b728"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-gmd9n0n_\n","  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-gmd9n0n_\n","Requirement already satisfied: keras in /tensorflow-1.15.2/python3.7 (from keras-contrib==2.0.8) (2.3.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.1.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.21.5)\n","Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.7 (from keras->keras-contrib==2.0.8) (1.0.8)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.5.2)\n"]}]},{"cell_type":"code","source":["!pip install -U tensorflow-addons"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hDNse7oGWjP9","executionInfo":{"status":"ok","timestamp":1648887484154,"user_tz":-480,"elapsed":5477,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"2d2fc09e-84e3-46af-8535-47d42e061832"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 194 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 256 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 276 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 307 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 337 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 358 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 368 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 389 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 440 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 450 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 471 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 481 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 501 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 512 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 522 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 532 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 542 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 552 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 563 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 573 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 583 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 604 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 614 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 624 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 634 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 645 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 655 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 675 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 686 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 696 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 706 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 716 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 727 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 737 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 747 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 757 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 768 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 778 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 788 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 798 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 808 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 819 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 829 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 849 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 860 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 870 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 880 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 890 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 901 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 911 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 921 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 931 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 942 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 952 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 962 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 972 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 983 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 993 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.0 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 11.9 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.16.1\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from keras.models import Model, Input\n","from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n","from keras_contrib.layers import CRF"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGAIFNsWJh-N","executionInfo":{"status":"ok","timestamp":1648899234424,"user_tz":-480,"elapsed":364,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"2f9befcf-3131-404b-ca03-77971e3fb18f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"Y7KJjVCVWIyz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_file = '/content/drive/MyDrive/ABSA_data/Restaurants_Train.xml'\n","# test_file = '/content/drive/MyDrive/ABSA_data/Restaurants_Test.xml'\n","\n","train_file = '/content/drive/MyDrive/ABSA_data/Laptops_Train.xml'\n","test_file = '/content/drive/MyDrive/ABSA_data/Laptops_Test.xml'"],"metadata":{"id":"ItDVtuMuBjUk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def xml_to_dataframe(filepath):\n","  data = []\n","\n","  with open(filepath, 'r') as xml_file:\n","    tree = ET.parse(xml_file)\n","    sentences = tree.getroot()\n","\n","    for sent in sentences:\n","      record = dict()\n","      record[\"id\"] = sent.attrib['id']\n","      record[\"text\"] = sent.findall(\".//text\")[0].text\n","      record[\"aspectTerms\"] = []\n","      # record[\"aspectCats\"] = []\n","\n","      aspectTerms = sent.findall(\".//aspectTerms\")\n","      if aspectTerms:\n","        record[\"aspectTerms\"] = [term.attrib for term in sent.findall(\".//aspectTerms\")[0]]\n","\n","      # aspectCats = sent.findall(\".//aspectCategories\")\n","      # if aspectCats:\n","      #   record[\"aspectCats\"] = [cat.attrib for cat in sent.findall(\".//aspectCategories\")[0]]\n","\n","      data.append(record)\n","  return pd.DataFrame(data)"],"metadata":{"id":"TDCqt8oTBvuv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = xml_to_dataframe(train_file)"],"metadata":{"id":"CaBO0RTxBnsG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"8ucfta1fDHOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens_list = []\n","for index, row in train_df.iterrows():\n","  sentence_doc = nlp(row['text'])\n","  tokens = list(sentence_doc)\n","  tokens_list.append(tokens)"],"metadata":{"id":"TRUaI86XNZLf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flat_list = [item.text for sublist in tokens_list for item in sublist]\n","vocab = set(flat_list)\n","# idx 0 for unknown word\n","word2idx = {w: i + 1 for i, w in enumerate(vocab)}\n","tag2idx = {\"PAD\":0, \"O\":1, \"I\":2, \"B\":3}"],"metadata":{"id":"puyC1iLbIpjk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_idx_iob(idx, range_list):\n","  # Use IOB2 format\n","  for range in range_list:\n","    if range[0] <= idx < range[1]:\n","      if range[0] == idx:\n","        return 'B'\n","      else:\n","        return 'I'\n","  return 'O'\n","\n","def get_train_inputs(xml_df):\n","  train_x = []\n","  train_y = []\n","\n","  for index, row in xml_df.iterrows():\n","    # A list of tuples, each tuple is of (<from>, <to>)\n","    aspectTermRanges = [(int(aspectTerm['from']), int(aspectTerm['to'])) for aspectTerm in row['aspectTerms']]\n","    sentence_doc = nlp(row['text'])\n","    train_x.append([word2idx.get(w.text,0) for w in sentence_doc])\n","    train_y.append([tag2idx[get_idx_iob(w.idx, aspectTermRanges)] for w in sentence_doc])\n","  return train_x, train_y"],"metadata":{"id":"bnXxVnyDCOq5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x, train_y = get_train_inputs(train_df)"],"metadata":{"id":"6LL70KjURbNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = max(len(idxs) for idxs in train_x )\n","\n","train_x_pad = pad_sequences(maxlen=max_len, sequences=train_x, padding=\"post\", value=0)\n","train_y_pad = pad_sequences(maxlen=max_len, sequences=train_y, padding=\"post\", value=0)\n","# Get one-hot labels\n","train_y_onehot = [to_categorical(i, num_classes=4) for i in train_y_pad]\n","\n","train_x_pad = np.array(train_x_pad)\n","train_y_onehot = np.array(train_y_onehot).astype(int)"],"metadata":{"id":"lzSeKk0-RzZ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras import backend as K\n","def get_f1(y_true, y_pred): #taken from old keras source code\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    return f1_val"],"metadata":{"id":"DPZ-37Q3oqJQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model\n","input = Input(shape=(max_len,))\n","model = Embedding(input_dim=len(word2idx) + 1, output_dim=20,\n","                  input_length=max_len, mask_zero=False)(input)  # 20-dim embedding\n","model = Bidirectional(LSTM(units=50, return_sequences=True,\n","                           recurrent_dropout=0.1))(model)  # variational biLSTM\n","model = TimeDistributed(Dense(50, activation=\"tanh\"))(model)  # a dense layer as suggested by neuralNer\n","crf = CRF(4)  # CRF layer\n","out = crf(model)  # output\n","\n","model = Model(input, out)\n","model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z7SWf5MVSxkF","executionInfo":{"status":"ok","timestamp":1648900350052,"user_tz":-480,"elapsed":2859,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"e0eebe46-9932-4bfc-a9ca-52252cadc29d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_7\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_7 (InputLayer)         (None, 84)                0         \n","_________________________________________________________________\n","embedding_7 (Embedding)      (None, 84, 20)            104500    \n","_________________________________________________________________\n","bidirectional_7 (Bidirection (None, 84, 100)           28400     \n","_________________________________________________________________\n","time_distributed_7 (TimeDist (None, 84, 50)            5050      \n","_________________________________________________________________\n","crf_7 (CRF)                  (None, 84, 4)             228       \n","=================================================================\n","Total params: 138,178\n","Trainable params: 138,178\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["history = model.fit(train_x_pad, train_y_onehot, batch_size=32, epochs=8,\n","                    validation_split=0.1, verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IML5Vb5HYED2","executionInfo":{"status":"ok","timestamp":1648900452827,"user_tz":-480,"elapsed":94915,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"ade9f962-0cb4-4602-a5d1-ea296d21bdef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train on 2743 samples, validate on 305 samples\n","Epoch 1/8\n","2743/2743 [==============================] - 13s 5ms/step - loss: 0.1973 - crf_viterbi_accuracy: 0.9134 - val_loss: 0.0939 - val_crf_viterbi_accuracy: 0.9738\n","Epoch 2/8\n","2743/2743 [==============================] - 11s 4ms/step - loss: 0.0732 - crf_viterbi_accuracy: 0.9803 - val_loss: 0.0637 - val_crf_viterbi_accuracy: 0.9811\n","Epoch 3/8\n","2743/2743 [==============================] - 11s 4ms/step - loss: 0.0489 - crf_viterbi_accuracy: 0.9847 - val_loss: 0.0433 - val_crf_viterbi_accuracy: 0.9845\n","Epoch 4/8\n","2743/2743 [==============================] - 11s 4ms/step - loss: 0.0391 - crf_viterbi_accuracy: 0.9854 - val_loss: 0.0387 - val_crf_viterbi_accuracy: 0.9853\n","Epoch 5/8\n","2743/2743 [==============================] - 11s 4ms/step - loss: 0.0322 - crf_viterbi_accuracy: 0.9868 - val_loss: 0.0316 - val_crf_viterbi_accuracy: 0.9869\n","Epoch 6/8\n","2743/2743 [==============================] - 11s 4ms/step - loss: 0.0274 - crf_viterbi_accuracy: 0.9883 - val_loss: 0.0289 - val_crf_viterbi_accuracy: 0.9875\n","Epoch 7/8\n","2743/2743 [==============================] - 11s 4ms/step - loss: 0.0246 - crf_viterbi_accuracy: 0.9885 - val_loss: 0.0267 - val_crf_viterbi_accuracy: 0.9873\n","Epoch 8/8\n","2743/2743 [==============================] - 11s 4ms/step - loss: 0.0214 - crf_viterbi_accuracy: 0.9894 - val_loss: 0.0243 - val_crf_viterbi_accuracy: 0.9876\n"]}]},{"cell_type":"code","source":["test_df = xml_to_dataframe(test_file)\n","test_x, text_y = get_train_inputs(test_df)\n","\n","test_x_pad = pad_sequences(maxlen=max_len, sequences=test_x, padding=\"post\", value=0)\n","test_y_pad = pad_sequences(maxlen=max_len, sequences=text_y, padding=\"post\", value=0)"],"metadata":{"id":"_vOhOaOvZI0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_predict = model.predict(test_x_pad)\n","p_all_label= np.argmax(test_predict, axis=-1) "],"metadata":{"id":"xOpL7XPRhxi8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx2word = {value: key for key, value in word2idx.items()}\n","idx2tag = {value: key for key, value in tag2idx.items()}\n","\n","true_all_tags = [[idx2tag[idx] for idx in s if idx!=0] for s in test_y_pad]\n","p_all_tags = [[idx2tag[idx] for idx in s] for s in p_all_label]\n","\n","for i, true in enumerate(true_all_tags):\n","    length = len(true)\n","    p_all_tags[i] = p_all_tags[i][:length]\n","\n","p_all_tags = [[x.replace('PAD', 'O') for x in s] for s in p_all_tags]"],"metadata":{"id":"ZERkRvfYjCOf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluation\n","from seqeval.metrics import f1_score, classification_report\n","print(f1_score(true_all_tags, p_all_tags))\n","print(classification_report(true_all_tags, p_all_tags))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6LjcO4lGiPsa","executionInfo":{"status":"ok","timestamp":1648900496857,"user_tz":-480,"elapsed":2,"user":{"displayName":"Zhenghao Wu","userId":"02686925948298387783"}},"outputId":"f0e053d8-4085-47f1-e18d-285a63d790e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.38716356107660455\n","              precision    recall  f1-score   support\n","\n","           _       0.60      0.29      0.39       653\n","\n","   micro avg       0.60      0.29      0.39       653\n","   macro avg       0.60      0.29      0.39       653\n","weighted avg       0.60      0.29      0.39       653\n","\n"]}]},{"cell_type":"code","source":["!pip install seqeval"],"metadata":{"id":"IdkHJhWwibYT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Z0ESPLmhiV8A"},"execution_count":null,"outputs":[]}]}